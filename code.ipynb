{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Group 1\n",
    "\n",
    "## Alireza Mousavizadeh - 97106284\n",
    "\n",
    "## Fatemeh Tohidian - 97100354\n",
    "\n",
    "## Amin Kashiri - 97101026"
   ],
   "metadata": {
    "id": "kgfuam-PjmSM",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {
    "id": "Pjv4EN2tkOfI",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install Packages\n",
    "\n",
    "install required packages using requirement.txt file."
   ],
   "metadata": {
    "id": "xILwINHKQLmi",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install torch\n",
    "# !pip install evaluate\n",
    "# !pip install wandb -qU"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rOPpH9ctSyt",
    "outputId": "4ed1aab5-185d-4cc8-cc21-e442fc9cbe05",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {
    "id": "qN5z92j_QfUp",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import json\n",
    "import evaluate as e\n",
    "# from google.colab import drive\n",
    "from transformers import AutoTokenizer, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import SGD\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"1d6bdaf3f9f088abf0915e5e5cb6689e4c7e7476\""
   ],
   "metadata": {
    "id": "dsQiuyv6Qi-f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahur4/anaconda3/envs/data_env/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check whether cuda is available\n",
    "\n",
    "Check whether cuda is available and based on this, device object is built that is used in for pytorch tensors computation."
   ],
   "metadata": {
    "id": "2qF4bqKjQuhD",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ],
   "metadata": {
    "id": "_NQRW2bwcJul",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-Parameter Setting\n",
    "\n",
    "In this section, hyper-parameters that used in bert fine-tuning are defined. hyper-parameter optimization (HPO) will be done in the next parts.\n"
   ],
   "metadata": {
    "id": "P1Ifda76RCPI",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "EXPERIMENT_NUM = 3\n",
    "MAX_LEN = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "BIO = True"
   ],
   "metadata": {
    "id": "C1DqfQLTHNyy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise WANDB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33msamousavizade\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.12.21"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/ahur4/NLP5/NLP-HW5(1)/wandb/run-20220731_231139-fcyy3dxm</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/samousavizade/NER-Detection/runs/fcyy3dxm\" target=\"_blank\">experiment_3</a></strong> to <a href=\"https://wandb.ai/samousavizade/NER-Detection\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "  project=\"NER-Detection\",\n",
    "  name=f\"experiment_{EXPERIMENT_NUM}\",\n",
    "  config={\n",
    "      \"experiment\": EXPERIMENT_NUM,\n",
    "      \"learning_rate\": LEARNING_RATE,\n",
    "      \"batch_size\": BATCH_SIZE,\n",
    "      \"BIO\": BIO,\n",
    "      \"epochs\": EPOCHS,\n",
    "      \"max_len\": MAX_LEN,\n",
    "  })\n",
    "\n",
    "config = wandb.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data\n",
    "\n",
    "load train, validation, test dataset with json."
   ],
   "metadata": {
    "id": "5_DFsqpsQ1uE",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# path = \"/content/drive/MyDrive/Colab Notebooks/dataset_annotated_splited.json\"\n",
    "# path = \"/content/drive/MyDrive/NLP/HW5/dataset_annotated_splited.json\"\n",
    "path = \"dataset_annotated_splited.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    train_data = data[\"train\"][:]\n",
    "    test_data = data[\"test\"][:]\n",
    "    val_data = data[\"eval\"][:]\n",
    "\n",
    "print(train_data[0].keys())\n",
    "print(train_data[0]['annotations'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCLWn714wsYA",
    "outputId": "3bafc21c-3726-4a83-cba3-598798d1d636",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['header', 'text', 'annotations'])\n",
      "[{'header': [{'name': 'DAT', 'range': [27, 37]}], 'text': [{'name': 'ORG', 'range': [9, 24]}, {'name': 'ORG', 'range': [32, 49]}, {'name': 'PER', 'range': [51, 62]}, {'name': 'ORG', 'range': [68, 79]}, {'name': 'ORG', 'range': [153, 164]}, {'name': 'PER', 'range': [210, 222]}, {'name': 'DAT', 'range': [269, 280]}, {'name': 'ORG', 'range': [349, 360]}, {'name': 'PER', 'range': [369, 375]}, {'name': 'PER', 'range': [414, 426]}, {'name': 'TIM', 'range': [478, 485]}, {'name': 'DAT', 'range': [465, 470]}, {'name': 'ORG', 'range': [510, 521]}]}, {'header': [], 'text': [{'name': 'PER', 'range': [51, 62]}, {'name': 'PER', 'range': [210, 222]}, {'name': 'PER', 'range': [369, 375]}, {'name': 'PER', 'range': [414, 426]}]}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label to ID Mapping\n",
    "\n",
    "In this section, labels to ids and ids to labels are built for next usage in bert fine-tuning training."
   ],
   "metadata": {
    "id": "pAe0OTK6Q6eP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if config.BIO:\n",
    "    label_list = [\n",
    "        \"O\", \n",
    "        \"B_ORG\", \"B_PER\", \"B_DAT\", \"B_TIM\", \"B_LOC\", \"B_EVE\", \"B_mainLOC\", \"B_NAT\",\n",
    "        \"I_ORG\", \"I_PER\", \"I_DAT\", \"I_TIM\", \"I_LOC\", \"I_EVE\", \"I_mainLOC\", \"I_NAT\"\n",
    "    ]\n",
    "else:\n",
    "    label_list = [\"O\", \"ORG\", \"PER\", \"DAT\", \"TIM\", \"LOC\", \"EVE\", \"mainLOC\", \"NAT\"]\n",
    "labels_to_ids = {k: v for v, k in enumerate(label_list)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(label_list)}\n",
    "\n",
    "LABELS = len(label_list)\n",
    "\n",
    "print(labels_to_ids)\n",
    "print(ids_to_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpPxIA66tX2f",
    "outputId": "3456f64e-bd14-499e-d171-aece61147b3c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B_ORG': 1, 'B_PER': 2, 'B_DAT': 3, 'B_TIM': 4, 'B_LOC': 5, 'B_EVE': 6, 'B_mainLOC': 7, 'B_NAT': 8, 'I_ORG': 9, 'I_PER': 10, 'I_DAT': 11, 'I_TIM': 12, 'I_LOC': 13, 'I_EVE': 14, 'I_mainLOC': 15, 'I_NAT': 16}\n",
      "{0: 'O', 1: 'B_ORG', 2: 'B_PER', 3: 'B_DAT', 4: 'B_TIM', 5: 'B_LOC', 6: 'B_EVE', 7: 'B_mainLOC', 8: 'B_NAT', 9: 'I_ORG', 10: 'I_PER', 11: 'I_DAT', 12: 'I_TIM', 13: 'I_LOC', 14: 'I_EVE', 15: 'I_mainLOC', 16: 'I_NAT'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise Bert Tokenizer\n",
    "\n",
    "In this section, **ParsBERT(v2.0)** tokenizer is used for tokenization. ParsBERT (v2.0) is a Transformer-based Model for Persian Language Understanding that reconstructed the vocabulary and fine-tuned the ParsBERT v1.1 on the new Persian corpora in order to provide some functionalities for using ParsBERT in other scopes! Follow the ParsBERT repo for the latest information about previous and current models. Persian Text Classification [DigiMag, Persian News] The task target is labeling texts in a supervised manner in both existing datasets DigiMag and Persian News. A total of 8,515 articles scraped from **Digikala** Online Magazine. This dataset includes seven different classes."
   ],
   "metadata": {
    "id": "d2UmVH0UQ9qo",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "krIY580imt54",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HooshvareLab/bert-fa-base-uncased-clf-digimag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Overlaps between Named Entity Tags\n",
    "\n",
    "In this section, some functions are defined to handle overlapping ner tags in such a way that the inner tags are removed and only the outermost tags are kept (Larger tag is keeped and smaller is removed)."
   ],
   "metadata": {
    "id": "5jXL7aYmRQHr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def has_intersection(first, second):\n",
    "    if first[0] < second[0]:\n",
    "        if first[1] <= second[0]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        if first[0] >= second[1]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def remove_annotation_overlap(annotations):\n",
    "    annotations = sorted(annotations, key=lambda x: x[\"range\"][0])\n",
    "    n = len(annotations)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while i < n and j < n:\n",
    "        first = annotations[i]\n",
    "        first_range = first[\"range\"]\n",
    "        second = annotations[j]\n",
    "        second_range = second[\"range\"]\n",
    "        if has_intersection(first_range, second_range):\n",
    "            new = first if (first_range[1]-first_range[0]) > (second_range[1]-second_range[0]) else second\n",
    "            annotations[i]= new\n",
    "            annotations[j] = None\n",
    "        else:\n",
    "            i = j\n",
    "        j += 1\n",
    "\n",
    "    annotations = list(filter(lambda x: not x is None, annotations))\n",
    "    return annotations\n"
   ],
   "metadata": {
    "id": "Kc-bscDARRA7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Character Level to Token Level Indexing\n",
    "\n",
    "In this section, some functions are defined to handle token level indexing. to overcome token level indexing, CLS and END must be considered. "
   ],
   "metadata": {
    "id": "GaTA08awRXoL",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gVni_uwPmt57",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_starting_token_index(tag_start, word_index, token_offsets):\n",
    "    while word_index <= config.max_len - 1 and token_offsets[word_index][0] < tag_start:\n",
    "        word_index += 1\n",
    "    return word_index\n",
    "\n",
    "def get_ending_token_index(tag_stop, word_index, token_offsets):\n",
    "    while (\n",
    "        word_index <= config.max_len - 1\n",
    "        and token_offsets[word_index][1] < tag_stop\n",
    "        and token_offsets[word_index][1] != 0\n",
    "    ):\n",
    "        word_index += 1\n",
    "    return word_index\n",
    "\n",
    "def get_final_label(encoding, annotation):\n",
    "    token_offsets = encoding[\"offset_mapping\"]\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    end_element = torch.argmin(input_ids)\n",
    "    final_labels = [-100] * config.max_len\n",
    "    final_labels[1:end_element] = [0] * (end_element - 1)\n",
    "\n",
    "    annotations_without_overlap = remove_annotation_overlap(annotation)\n",
    "\n",
    "    word_index = 1\n",
    "    for label in annotations_without_overlap:\n",
    "        interval = label[\"range\"]\n",
    "        label_name = label[\"name\"]\n",
    "        word_index = get_starting_token_index(interval[0], word_index, token_offsets)\n",
    "        start_token_index = word_index\n",
    "        if start_token_index == config.max_len:\n",
    "          break\n",
    "        word_index = get_ending_token_index(interval[1], word_index, token_offsets)\n",
    "        end_token_index = word_index\n",
    "        if config.BIO:\n",
    "            final_labels[start_token_index:end_token_index+1] = [labels_to_ids[\"I_\"+label_name]] * (end_token_index-start_token_index+1)\n",
    "            final_labels[start_token_index] = labels_to_ids[\"B_\"+label_name]\n",
    "        else:\n",
    "            final_labels[start_token_index:end_token_index+1] = [labels_to_ids[label_name]] * (end_token_index-start_token_index+1)\n",
    "\n",
    "        word_index += 1\n",
    "    return final_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define DataSequence and DataLoader\n",
    "\n",
    "In this section, DataSequence and DataLoader that used in bert fine-tuning are defined."
   ],
   "metadata": {
    "id": "yvcHk2pzRd2T",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DataSequence(torch.utils.data.Dataset):\n",
    "    def __init__(self, news_list):\n",
    "        labels = []\n",
    "        texts = []\n",
    "        for news in news_list:\n",
    "            header = news[\"header\"]\n",
    "            text = news[\"text\"]\n",
    "            header_annotaition = []\n",
    "            text_annotation = []\n",
    "            for i in range(len(news[\"annotations\"])): \n",
    "                header_annotaition.extend(news[\"annotations\"][i][\"header\"])\n",
    "                text_annotation.extend(news[\"annotations\"][i][\"text\"])\n",
    "\n",
    "            for t, annotation in [(header,header_annotaition), (text, text_annotation)]:\n",
    "                encoding = tokenizer(\n",
    "                    t,\n",
    "                    return_offsets_mapping=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=config.max_len, # including [CLS] end [SEP]\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                for key in ['input_ids', 'attention_mask', 'token_type_ids', 'offset_mapping']:\n",
    "                    encoding[key] = encoding[key][0]\n",
    "                label = get_final_label(encoding, annotation)\n",
    "                texts.append(encoding)\n",
    "                labels.append(label)\n",
    "\n",
    "\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ],
   "metadata": {
    "id": "4LcDX9HwRgai",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialise DataSequence and DataLoader Object"
   ],
   "metadata": {
    "id": "KoQSP7uuRmqN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_dataloader(data, batch=None, cuda=True):\n",
    "    dataset = DataSequence(data)\n",
    "    print('len ds: ', len(dataset))\n",
    "    if batch is None:\n",
    "        batch = len(dataset)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset, num_workers=1, batch_size=batch, shuffle=True\n",
    "    )\n",
    "    if cuda:\n",
    "        dataloader = DeviceDataLoader(dataloader, device)\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(train_data, config.batch_size)\n",
    "val_dataloader = get_dataloader(val_data, config.batch_size)\n",
    "test_dataloader = get_dataloader(test_data, config.batch_size)"
   ],
   "metadata": {
    "id": "msu7g3ip3vMk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22eb7929-8493-4f06-d487-5bc6fd6cd8bb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len ds:  2700\n",
      "len ds:  150\n",
      "len ds:  150\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypmb0kMIUTDi",
    "outputId": "794eedc4-acb0-486e-d9f9-05d10dac7e94",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "19\n",
      "19\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Bert NER Model"
   ],
   "metadata": {
    "id": "DgjWktTfRssP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "class BertNER(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertNER, self).__init__()\n",
    "        self.bert = BertForTokenClassification.from_pretrained(\n",
    "            \"HooshvareLab/bert-fa-base-uncased-clf-digimag\",\n",
    "            num_labels=LABELS,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_batch, labels):\n",
    "        input_ids = input_batch[\"input_ids\"]\n",
    "        mask = input_batch[\"attention_mask\"]\n",
    "        output = self.bert(\n",
    "            input_ids=input_ids, attention_mask=mask, labels=labels, return_dict=False\n",
    "        )\n",
    "        return output"
   ],
   "metadata": {
    "id": "07dJvyOAA5m0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertNER()\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElR4lGfzeGwg",
    "outputId": "04a4eb00-0869-4108-82af-49c71e732f03",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased-clf-digimag and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([17, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([17]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "id": "qwBe9h-SRvvC",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim import SGD, Adam\n",
    "\n",
    "def train_loop(model, train_dataloader, val_dataloader):\n",
    "    optimizer = SGD(model.parameters(), lr=config.learning_rate)\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss_train = 0\n",
    "        true_labels_list_train = []\n",
    "        predictions_list_train = []\n",
    "\n",
    "        model.train()\n",
    "        for input_batch, batch_labels in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(input_batch, batch_labels)\n",
    "            logits_clean = logits[batch_labels != -100]\n",
    "            true_labels = batch_labels[batch_labels != -100]\n",
    "\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predictions_list_train.append(predictions.tolist())\n",
    "            true_labels_list_train.append(true_labels.tolist())\n",
    "\n",
    "        prefix = \"train\"\n",
    "        train_metrics = evaluate_metrics(predictions_list_train, true_labels_list_train, prefix=prefix)\n",
    "\n",
    "        train_metrics[f\"{prefix}/loss\"] = total_loss_train / len(predictions_list_train)\n",
    "\n",
    "        wandb.log(train_metrics, step=epoch + 1)\n",
    "\n",
    "        total_loss_validation = 0\n",
    "        true_labels_list_validation = []\n",
    "        predictions_list_validation = []\n",
    "\n",
    "        model.eval()\n",
    "        for input_batch, batch_labels in val_dataloader:\n",
    "            loss, logits = model(input_batch, batch_labels)\n",
    "            logits_clean = logits[batch_labels != -100]\n",
    "            true_labels = batch_labels[batch_labels != -100]\n",
    "\n",
    "            predictions = logits_clean.argmax(dim=1)\n",
    "\n",
    "            total_loss_validation += loss.item()\n",
    "\n",
    "            predictions_list_validation.append(predictions.tolist())\n",
    "            true_labels_list_validation.append(true_labels.tolist())\n",
    "\n",
    "        prefix = \"validation\"\n",
    "        validation_metrics = evaluate_metrics(predictions_list_validation, true_labels_list_validation, prefix=prefix)\n",
    "\n",
    "        validation_metrics[f\"{prefix}/loss\"] = total_loss_validation / len(predictions_list_validation)\n",
    "\n",
    "        wandb.log(validation_metrics, step=epoch + 1)\n"
   ],
   "metadata": {
    "id": "iAMIoYTmBvZt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "For Evaluation task, evaluate library is used. Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized.\n",
    "\n"
   ],
   "metadata": {
    "id": "8HzQGnTDR52M",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from pprint import pprint\n",
    "\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def evaluate_metrics(predictions, label_clean, prefix):\n",
    "    metrics = {}\n",
    "\n",
    "    n = len(predictions)\n",
    "\n",
    "    for i in range(n):\n",
    "        f1_metric.add_batch(\n",
    "            references=label_clean[i],\n",
    "            predictions=predictions[i]\n",
    "        )\n",
    "    metrics[f'{prefix}/f1_macro'] = f1_metric.compute(average=\"macro\")[\"f1\"]\n",
    "\n",
    "    for i in range(n):\n",
    "        f1_metric.add_batch(\n",
    "            references=label_clean[i],\n",
    "            predictions=predictions[i]\n",
    "        )\n",
    "    metrics[f'{prefix}/f1_micro'] = f1_metric.compute(average=\"micro\")[\"f1\"]\n",
    "\n",
    "    for i in range(n):\n",
    "        f1_metric.add_batch(\n",
    "            references=label_clean[i],\n",
    "            predictions=predictions[i]\n",
    "        )\n",
    "    metrics[f'{prefix}/f1_weighted'] = f1_metric.compute(average=\"weighted\")[\"f1\"]\n",
    "\n",
    "    for i in range(n):\n",
    "        accuracy_metric.add_batch(\n",
    "            references=label_clean[i],\n",
    "            predictions=predictions[i]\n",
    "        )\n",
    "\n",
    "    metrics[f'{prefix}/accuracy'] = accuracy_metric.compute()[\"accuracy\"]\n",
    "\n",
    "    return metrics\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"training ...\")\n",
    "\n",
    "train_loop(model, train_dataloader, val_dataloader)"
   ],
   "metadata": {
    "id": "YXmYguOyGOry",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "14f9a3d6-bc92-4a91-b959-a4a0de53cc55",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 338/338 [01:38<00:00,  3.42it/s]\n",
      "100%|██████████| 338/338 [01:43<00:00,  3.27it/s]\n",
      "100%|██████████| 338/338 [01:39<00:00,  3.38it/s]\n",
      "100%|██████████| 338/338 [01:40<00:00,  3.37it/s]\n",
      "100%|██████████| 338/338 [01:42<00:00,  3.29it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "id": "ezGum4307_Ea",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "total_loss_test = 0\n",
    "true_labels_list_test = []\n",
    "predictions_list_test = []\n",
    "\n",
    "model.eval()\n",
    "for input_batch, batch_labels in test_dataloader:\n",
    "    loss, logits = model(input_batch, batch_labels)\n",
    "    logits_clean = logits[batch_labels != -100]\n",
    "    true_labels = batch_labels[batch_labels != -100]\n",
    "\n",
    "    predictions = logits_clean.argmax(dim=1)\n",
    "\n",
    "    total_loss_test += loss.item()\n",
    "\n",
    "    predictions_list_test.append(predictions.tolist())\n",
    "    true_labels_list_test.append(true_labels.tolist())\n",
    "\n",
    "prefix = \"test\"\n",
    "test_metrics = evaluate_metrics(predictions_list_test, true_labels_list_test, prefix=prefix)\n",
    "\n",
    "test_metrics[f\"{prefix}/loss\"] = total_loss_test / len(predictions_list_test)\n",
    "\n",
    "for metric, value in test_metrics.items():\n",
    "    wandb.run.summary[metric] = value"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dd08goaH6k8V",
    "outputId": "dc78c4e6-e36e-4632-8eb6-e36285741599",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finish WANDB Session\n",
    "\n",
    "results will be stored in wab cloud storage and can be retrieved any time from [this](https://wandb.ai/samousavizade/NER-Detection?workspace=user-samousavizade) link."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8365bfc14d342309fe9e62b8fa16959"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/accuracy</td><td>▁████</td></tr><tr><td>train/f1_macro</td><td>█▁▁▂▇</td></tr><tr><td>train/f1_micro</td><td>▁████</td></tr><tr><td>train/f1_weighted</td><td>▁▇▇▇█</td></tr><tr><td>train/loss</td><td>█▃▃▂▁</td></tr><tr><td>validation/accuracy</td><td>▁▁▁▂█</td></tr><tr><td>validation/f1_macro</td><td>▁▁▁▂█</td></tr><tr><td>validation/f1_micro</td><td>▁▁▁▂█</td></tr><tr><td>validation/f1_weighted</td><td>▁▁▁▂█</td></tr><tr><td>validation/loss</td><td>█▇▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy</td><td>0.64597</td></tr><tr><td>test/f1_macro</td><td>0.05614</td></tr><tr><td>test/f1_micro</td><td>0.64597</td></tr><tr><td>test/f1_weighted</td><td>0.51532</td></tr><tr><td>test/loss</td><td>1.35312</td></tr><tr><td>train/accuracy</td><td>0.64782</td></tr><tr><td>train/f1_macro</td><td>0.05164</td></tr><tr><td>train/f1_micro</td><td>0.64782</td></tr><tr><td>train/f1_weighted</td><td>0.51437</td></tr><tr><td>train/loss</td><td>1.42178</td></tr><tr><td>validation/accuracy</td><td>0.69845</td></tr><tr><td>validation/f1_macro</td><td>0.05572</td></tr><tr><td>validation/f1_micro</td><td>0.69845</td></tr><tr><td>validation/f1_weighted</td><td>0.58109</td></tr><tr><td>validation/loss</td><td>1.13195</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">experiment_3</strong>: <a href=\"https://wandb.ai/samousavizade/NER-Detection/runs/fcyy3dxm\" target=\"_blank\">https://wandb.ai/samousavizade/NER-Detection/runs/fcyy3dxm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20220731_231139-fcyy3dxm/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from transformers import pipeline\n",
    "# nlp = pipeline(\"ner\", model=model.bert.to('cpu'), tokenizer=tokenizer)\n",
    "# # example = \"حسین تقوی به سازمان جهاد کشاورزی رفت.\"\n",
    "# # example = 'امین به ایران آمد.'\n",
    "# example = \"به گزارش خبرنگار مهر، نماینده ولی فقیه در آذربایجان شرقی پیش از ظهر امروز در مراسم بزرگداشت یوم الله ۱۲ بهمن که در تالار اجتماعات مصلی اعظم امام خمینی ره برگزار شد گفت: مشکلات اقتصادی و تحریم ها در کشور وجود دارد اما باید قدردان انقلاب اسلامی ایران بود و به همین دلیل، باید توگه بیشتری به موفقیت های به دست آمده در طول دوران انقلاب اسلامی داشت. حجت الاسلام و المسلمین سید محمد علی آل هاشم ادامه داد: سرعت پیشرفت علم در ایران بعد از انقلاب اسلامی و هم اکنون، ۱۱ برابر دنیاست؛ امروزه جمهوری اسلامی ایران، هشتمین کشور تولید کننده اورانیوم ۲۰ درصد جهان است\"\n",
    "# # example = \"به گزارش برنا؛ تقریبا از اسفند ماه سال گذشته واکسیناسیون عمومی در کشور با واردات واکسن های خارجی کرونا انجام شد و این روند به صورتی بود که محموله های جدید واکسن پس از خریداری شدن به کشور وارد می شد و تزریق ها برای گروه های اولویت دار انجام می گرفت البته در این میان جهش های جدیدی از ویروس در کشور زیاد شد و در مقابل واردات واکسن های خارجی با مشکلاتی مواجه بود و مسیر این اقدام با پستی ها و بلندی های زیادی رو به رو شد اما در حال حاضر با وجود همه اتفاقات بنا به گفته مسئولان ستاد مقابله با کرونا دو هفته ای از برنامه واکسیناسیون عقب هستیم و دلیل اصلی این اتفاق محدودیت وجود واکسن است. مسعود یونسیان، استاد اپیدمیولوژی دانشگاه علوم پزشکی تهران در گفت وگو با خبرنگار برنا درباره خرید واکسن های خارجی توسط شرکت های خصوصی گفت: دولت از خرید واکسن شرکت های خصوصی استقبال می کند و اصولا بسیاری از کشور های دیگر نیز واردات واکسن های خارجی را به شرکت های خصوصی سپرده اند اما در نهایت تحویل وزارت بهداشت می شود\"\n",
    "# ner_results = nlp(example)\n",
    "# print(ner_results)\n",
    "# pprint(list(zip(\n",
    "#     list(map(lambda x: x['word'], ner_results)),\n",
    "#     list(map(lambda x: ids_to_labels[int(x['entity'].split('_')[1])], ner_results))\n",
    "# )))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8I_EU2p0vzW-",
    "outputId": "3d1ea38b-b7a5-4330-e5e2-b63d795d7666",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('nlp_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd029c74b3931211e259a6407e9cb636f4133dd1d46c169023899434202d8e33"
   }
  },
  "colab": {
   "name": "BERT_NER.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "xILwINHKQLmi",
    "d2UmVH0UQ9qo"
   ]
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}